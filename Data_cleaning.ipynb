{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSKad3D5TetJ"
      },
      "source": [
        "**ESSENTIAL CLEANING TO BE FOLLOWED IN EVERY DATASET** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3idg64IJ63L"
      },
      "source": [
        "#modules required\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import chardet\r\n",
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN7a_75gbL12"
      },
      "source": [
        "#Mount the dataset from the drive\r\n",
        "#Folder Name - stands for the required dataset folder name created in the drive \r\n",
        "%cd '/content/drive/My Drive/Colab Notebooks/Folder Name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BI6ZRZb3vh"
      },
      "source": [
        "#Displays the first 5(defualt) rows in the dataset\r\n",
        "#filename - stands for the csv file name in the drive folder\r\n",
        "df = pd.read_csv('filename.csv',encoding= 'unicode_escape')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNXqk1wqeQIH"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#Removal of rows containing missing values(part 1)\r\n",
        "#Get the number of missing data points per column\r\n",
        "missing_values = df.isnull().sum()\r\n",
        "\r\n",
        "#look at the missing points in the first 5 columns\r\n",
        "missing_values[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x9HLOlPfmu0"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#Removal of rows containing missing values(part 2)\r\n",
        "#To find total missing values in our dataset\r\n",
        "total_cells = np.product(df.shape)\r\n",
        "total_missing_values = missing_values.sum()\r\n",
        "\r\n",
        "#Percentage of data missing\r\n",
        "percent_missing = (total_missing_values/total_cells) * 100\r\n",
        "print(percent_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J-Lw3iBf23o"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#Removal of rows containing missing values(part 3)\r\n",
        "#Remove all the rows that contain atleast 1  missing value\r\n",
        "dropped_rows=df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_qWrvCNhAH2"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#Removal of rows containing missing values(part 4)\r\n",
        "#Crosscheck\r\n",
        "#Get the number of missing data points per column\r\n",
        "missing_values = df.isnull().sum()\r\n",
        "\r\n",
        "#Look at the missing points in the first 5 columns\r\n",
        "missing_values[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6MivrHxKAd_"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#Instead of dropping rows conatining NaN as their value \r\n",
        "#We can instead find the mean of the remaining values in the column and insert the mean value instead of NaN\r\n",
        "#This is only possible if the column is numeric\r\n",
        "#Finding mean\r\n",
        "#ColumnName - stands for name of the column whose mean needs to be found\r\n",
        "\r\n",
        "df['ColumnName'] = df['ColumnName'].fillna(df.ColumnName.mean())\r\n",
        "df.head()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ6fJdjDdRu1"
      },
      "source": [
        "#Handling of NaN values in the dataset\r\n",
        "#When observations have some logical order in them we can use bfill or ffill to fill the missing values\r\n",
        "#Replace all NA's the value that comes directly before it in the same column, \r\n",
        "#Then replace all the remaining na's with with values that come after it in the same column\r\n",
        "#Bfill and FFill method\r\n",
        "df.fillna(method='ffill').fillna(method='bfill')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFhmREe5aB7X"
      },
      "source": [
        "#Handling of duplicate values in the dataset\r\n",
        "#Duplicates(part1)\r\n",
        "#To find if any duplicate rows exist\r\n",
        "duplicates=df[df.duplicated()]\r\n",
        "print(duplicates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCCURM3hcoFd"
      },
      "source": [
        "#Handling of duplicate values in the dataset\r\n",
        "#Duplicates(part2)\r\n",
        "#To remove duplicate rows\r\n",
        "df = df.drop_duplicates()\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoU7QEttUNwY"
      },
      "source": [
        "**OPTIONAL CLEANING WHICH IS DATASET SPECIFIC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDVkIdsAdl4e"
      },
      "source": [
        "#To avoid autogenerated unique indexes for each row we can set a column which has unique values for each row as the unique identifier if at all it exists\r\n",
        "#Unique Identifier(part 1)\r\n",
        "#To verify if a column is unique column\r\n",
        "#ColumnName - Name of the column to be checked if it is unique or not\r\n",
        "df['ColumnName'].is_unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xLiVBSgeCZi"
      },
      "source": [
        " #Unique Identifier(part 2)\r\n",
        " #To set the verified column as the unique column\r\n",
        " #ColumnName - Name of the column to be set as unique\r\n",
        " df = df.set_index('ColumnName')\r\n",
        " df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o6TuZ0O8IMH"
      },
      "source": [
        "#Some columns are already classified to have true or false values\r\n",
        "#Removal of rows which are classified as false\r\n",
        "#Here 1-true 0-false\r\n",
        "#ColumnName - Name of the classified column\r\n",
        "df_false = df[df['ColumnName']==0].index\r\n",
        "df.drop(df_false,inplace=True)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcelCF1uUIde"
      },
      "source": [
        "#Conversion of datatypes\r\n",
        "#To change the datatype of a column(part 1)\r\n",
        "#Incase it is not in its standard datatype\r\n",
        "#Req_datatype - datatype to be converted to\r\n",
        "#ColumnName,ColumnName1,ColumnName2 - names of the columns whose datatypes need to be changed\r\n",
        "\r\n",
        "#To check the datatype of a column\r\n",
        "df['ColumnName'].dtype\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtCn58V3RnlC"
      },
      "source": [
        "#To change the datatype of a column(part 2)\r\n",
        "df['ColumnName'] = df['ColumnName'].astype('Req_datatype')\r\n",
        "#For more than 1 column\r\n",
        "df= df.astype({'ColumnName1': 'Req_datatype', 'ColumnName2': 'Req_datatype'})\r\n",
        "#Crosscheck\r\n",
        "df['ColumnName'].dtype\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NMGfJ2-UUXS"
      },
      "source": [
        "#Handling of columns containing locations\r\n",
        "#Some values are displayed to be unique inspite of refering to the same city or country\r\n",
        "#This is due to the uppercase and lowercase difference or due to extra space after or between the words which requires cleaning\r\n",
        "#Locations(part 1)\r\n",
        "#location - stands for the column name\r\n",
        "\r\n",
        "\r\n",
        "# get all the unique values in the 'Country' column\r\n",
        "locations = df['location'].unique()\r\n",
        "\r\n",
        "# sort them alphabetically and then take a closer look\r\n",
        "#you will find values refering to same city or country displayed to be unique\r\n",
        "locations = \"\".join(sorted(locations))\r\n",
        "locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtYHcZ2lhMTb"
      },
      "source": [
        "#Locations(part 2)\r\n",
        "#When two fields having same value are displayed to be unique bcoz they are case sensitive or bcoz of extra spaces at the end\r\n",
        "#They can be converted to lowercase and spaces between the values can be removed\r\n",
        "#Convert to lower case\r\n",
        "df['location'] = df['location'].str.lower()\r\n",
        "#Remove trailing white spaces\r\n",
        "df['location'] = df['location'].str.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNlTZvp1tnDP"
      },
      "source": [
        "#Handling of irrelevant columns in the dataset\r\n",
        "#To remove irrelevant columns which are of no use as per the problem being solved\r\n",
        "#ColumnName,ColumnName1,ColumnName2 - Names of the column to be dropped\r\n",
        "df=df.drop(columns='ColumnName')\r\n",
        "#To drop more than 1 column\r\n",
        "df=df.drop(columns=['ColumnName1','ColumnName2'])\r\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlFmCZeLBn-j"
      },
      "source": [
        "#Handling of columns with dates\r\n",
        "#All dates can be displayed in the same datatype and in the same fashion i.e m/d/y\r\n",
        "#Dates(part1)\r\n",
        "#Print the first few rows of the date column\r\n",
        "print(df['date'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0llXINBA7CL"
      },
      "source": [
        "#Dates(part 2)\r\n",
        "#Check the data type of our date column\r\n",
        "df['date'].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8CAp_djBRnM"
      },
      "source": [
        "#Dates(part 3)\r\n",
        "#Create a new column, date_parsed, with the parsed dates\r\n",
        "#Here the datatype of the date column will change from object to datetime64 which is the actual datatype we need to have for dates\r\n",
        "df['date_parsed'] = pd.to_datetime(df['date'], format=\"%m/%d/%y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoibPCKHBbgc"
      },
      "source": [
        "#Dates(part 4)\r\n",
        "#Crosscheck\r\n",
        "#Print the first few rows\r\n",
        "df['date_parsed'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuBxEflYHFwg"
      },
      "source": [
        "#Handling of complex column names\r\n",
        "#To rename a complex column name\r\n",
        "# OriginalName - old name of the column & NewName - new name to be assigned\r\n",
        "df.rename(columns={'OrginalName':'NewName'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnz8RiNxIT8d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmvjz0D_YUaC"
      },
      "source": [
        "#Handling of outliers\r\n",
        "#outliers(part1)\r\n",
        "#plot_name - name of the plot to be used(Box,Scatter)\r\n",
        "#x_coordinate - column to be assigned on the x axis\r\n",
        "#y_coordinate - column to be assigned on the y axis\r\n",
        "df.plot(kind='plot_name' , x='x_coordinate' , y='y_coordinate' , rot = 70)\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoQ8-huJYhI2"
      },
      "source": [
        "#outliers(part2)\r\n",
        "#Remove outliers\r\n",
        "#value - value above which points should not be considered\r\n",
        "df_removed_outliers = df[df.y_coordinate<value]\r\n",
        "df_removed_outliers.plot(kind='plot_name', x='x_coordinate' , y='y_coordinate' , rot = 70)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcDvr2UjcDRu"
      },
      "source": [
        "#To check the final cleaned dataset\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}